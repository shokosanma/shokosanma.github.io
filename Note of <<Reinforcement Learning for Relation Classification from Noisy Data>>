摘要
现有的依赖于远程监督的关系分类方法假设一组包含实体的句子一直在描述实体关系。这种方法在句包级别进行分类，不能识别关系和句子之间的映射，而且很大程度上存在标签噪声问题。本文提出了一种新的基于噪声数据的句子级关系分类模型。该模型有两个模块：实例选择器和关系分类器。实例选择器通过强化学习选择高质量句子，将所选句子输入关系分类器，关系分类器进行句子级预测，并为实例选择器提供奖励。两个模块共同训练，以优化实例选择和关系分类过程。实验结果表明，该模型能有效地处理数据的噪声，在句子层次上对关系分类有较好的效果。
介绍
关系分类，旨在对给定纯文本的两个实体之间的语义关系进行分类，是自然语言处理中的一个重要问题，特别是对于知识图构建和问答。大多数现有的关系分类工作都采用有监督的学习方法，要么基于传统的手工制作特征（Mooney和Bunescu 2005; Zhou等人2005），要么基于深度神经网络自动生成的特征（Zeng et al.2014; dos Santos， Xiang和Zhou 2015），但都需要高质量的标注数据。为了获得大规模的训练数据，提出了远程监督（Mintz et al.2009，“远程监督”是一种学习方案，其中在给定弱标记训练集的情况下学习分类器（训练数据基于启发式/规则自动标记）。），假设如果两个实体在给定的知识库中具有关系，则包含这两个实体的所有句子都将提及该关系。 尽管远程监控对于自动标记数据是有效的，但是它存在噪声标记问题。以（Barack Obama，BornIn，美国）为例，嘈杂的句子“Barack Obamba是美国第44任总统”将被视为远程监督的积极事例，并将BornIn关系分配给这句话，虽然这句话根本没有描述BornIn的关系。
为了解决噪声标记问题，以前的研究采用多实例学习来考虑实例的噪声（Riedel，Yao和McCallum 2010; Hoffmann等人2011; Surdeanu等人2012; Zeng等人2015; Lin等人 al。2016; Ji et al.2017）。在这些研究中，训练和测试过程在句包级进行，其中句包包含提及相同实体对、但可能不描述相同关系的噪声句子。 因此，以前的研究受到两个局限：1）无法处理句子级预测; 2）对包中含有的根本没有描述实体关系的噪音句子敏感。
为了更好地解释第一个限制，我们在图1中给出了一个例子。句子包级预测可以在实体对“Barack Obama”和“United States”之间找到两个关系“EmployedBy”和“BornIn”。然而，句子级预测能够进一步将每个关系映射到对应的句子。至于第二个限制，对于每个句子包，先前的包级方法保留至少一个句子，即使给定包中的所有句子都是有噪声的（即没有描述该关系）。这种由远程监督生产的袋子很常见。例如，我们对广泛使用的数据集1的调查显示，100个样本袋中有53％没有描述实体关系的句子。这种存在噪声的句子袋肯定会降低关系分类的表现。
在本文中，为了处理上述两个局限性，我们提出了一种新的关系分类模型，它由两个模块组成：实例选择器和关系分类器。通过一个显式的实例选择器，我们能够首先从句子包中选择高质量句子，然后通过关系分类器预测句子级别的关系。为了处理第二个限制，如果所有句子都标记错误，我们的实例选择器将过滤整个包。 这里的主要挑战是如何联合训练两个模块，特别是当实例选择器没有明确知道哪些句子被错误标记时。
我们通过将实例选择任务作为强化学习问题来解决这一挑战（Sutton and Barto 1998）。直观地说，虽然我们没有对实例选择器进行明确的监督，但我们可以测量所选句子的整体效用。 因此，实例选择过程具有以下两个属性：第一，尝试和错误搜索，意味着实例选择器尝试选择一些句子并从关系分类器中获得对所选句子的质量的反馈（或奖励）。第二，只有当我们完成实例选择过程时，才能获得来自关系分类器的反馈，这通常是延迟的。这两个属性自然激励我们使用强化学习技术。
我们工作的贡献有：
我们提出了一种新的关系分类模型，它由实例选择器和关系分类器组成。这种形式化使我们的模型能够在清理后的数据的句子级别上提取关系。
我们将实例选择表示为强化学习问题，这使得模型能够在没有明确的句子级别注释的情况下执行实例选择，但只能使用来自关系分类器的弱监督信号。
相关工作
。。。
方法
我们提出了一种新的关系分类框架，它能够从噪声数据中选择正确的句子，以便更好地进行关系分类。该框架可以从清理好的数据中的句子级别，而不是句袋级别预测实体关系。句子级预测对于需要理解句子（例如问答和语义解析）的任务更加友好。
我们的框架由两个关键模块组成：实例选择器从噪声数据中选择正确的句子，以及关系分类器，它预测关系并用清理数据更新其参数。 这两个模块在培训过程中相互交互。
问题定义
在形式上，我们将关系分类的任务分解为本文中的两个子问题：实例选择和关系分类。
我们定义实例选择问题如下：给出一些形如<句子，关系分类>的对，作为 X = {(x1, r1), (x2, r2), ... , (xn, rn)}, 其中xi是与两个实体（hi，ti）相关联的句子，ri是由远程监督产生的嘈杂关系标签。目标是确定哪个句子真正描述了关系，并应选择作为训练实例。
关系分类问题定义如下：给定句子xi和提到的实体对（hi，ti），目标是预测xi中的语义关系ri。模型本质上是一组条件概率，即在xi,hi,ti确定的情况下，ri的概率pΦ（ri | xi，hi，ti）。
概览
所提出的模型基于强化学习框架，由两部分组成：实例选择器和关系分类器。在实例选择器中，每个句子xi具有相应的动作ai，以指示是否将xi选择为关系分类的训练实例。状态si由当前句子xi，{x1，...，xi-1}中已经选择的句子以及句子xi中的实体对hi和ti表示。实例选择器根据随机策略对给定当前状态的动作进行采样。对于关系分类器，它采用卷积体系结构来自动确定给定句子中实体对的语义关系。实例选择器将训练数据提取到关系分类器以训练卷积神经网络。同时，关系分类器向实例选择器提供反馈以改进其策略功能。图2给出了拟议框架如何工作的说明。
在实例选择器的帮助下，我们的方法直接滤除了嘈杂的句子。 与降低嘈杂句子的权重（Lin et al.2016）或保留一个句子（Zeng et al.2014）不同，我们的方法更好地处理噪声数据。 关系分类器在清理数据的句子级别进行训练和测试，而之前的模型将句子包作为一个整体来处理并预测包级别的关系。

实例选择器
我们将实例选择作为强化学习问题。 实例选择器是代理，它与包含数据和关系分类器的环境进行交互。代理根据策略决定在每一个状态（当前句子组成，被选择的句子集和实体对）时是否选择当前句子，之后从做完所有选择的最终状态的关系分类器处获得奖励。
如上所述，只有当所有训练实例的选择完成时，我们才能从关系分类器获得延迟奖励。因此，我们只能针对整个训练数据的每次扫描更新一次策略功能，这显然是低效的。为了获得更多反馈并使训练过程更有效，我们将训练句子实例X = {x1，...，xn}分成N个包B = {B1，B2，...，BN}并计算当我们在一个袋子里完成数据选择时返回的奖励。每个包对应于不同的实体对，并且每个包Bk是具有相同关系标签rk的句子{xk1，xk2，...，xk | Bk |}的序列，然而，关系标签是有噪声的。我们把动作定义为根据策略方法是否选择一个句子。一旦在一个包上完成选择决定，就会计算奖励。当实例选择器的训练过程完成后，我们将每个包中的所有选定语句合并，以获得一个已清理的数据集X^。然后，将清除的数据用于在句子级别训练关系分类器。
我们将介绍（即状态，行动和奖励）如下。为了清楚起见，我们将省略句子袋的上标ｋ。以下的公式都是基于一个袋子。
State状态
状态si表示在对包B的第i个句子做出决定时的当前句子，已经选择的句子，以及实体对。我们将状态表示为连续的实值向量F（si），其编码为以下信息：1）当前句子的向量表示，它是从用于关系分类的CNN的非线性层获得的;2）所选句子集的表示，它是所有选择句子的矢量表示的平均值;3）从预训练的知识图嵌入表获得的句子中两个实体的矢量表示。
Action动作
我们定义动作ai∈{0,1}以指示实例选择器是否将选择包B的第i个句子。 我们通过其策略函数πΘ（si，ai）对ai的值进行采样，其中Θ是要学习的参数。 在这项工作中，我们采用逻辑函数作为策略函数：
πΘ（si，ai）＝ PΘ(ai|si) = aiσ(W*F(si)+b) + (1-ai)(1-σ(W*F(si)+b))
上式中，F(si)是状态特征向量，σ(.)是参数为Θ = {W, b}的sigmoid函数。
Reward奖励
奖励函数是所选句子效用的指标。对于特定的包B = {x1，...，x | B |}，我们对每个句子采样一个动作，以确定是否应该选择当前句子。我们假设模型在完成所有选择后都有最终奖励。 因此，我们只在最终状态s | B | +1处收到延迟奖励。 其他状态的奖励为零。 因此，奖励定义如下：
r(si|B) = 1/|B^|*(sum(log(p(r|xj))) for xj in B^) if i = |B|+1 else 0
上式中，B^是全部句子B集合的子集，r是集合B的实体关系种类。p(r|xj)由一个CNN的关系分类器给出。对于B^＝空集的特例，我们将奖励设置为训练数据中的所有句子的平均似然估计。这使我们的实例选择器能够有效地排除噪音数据。
注意，关系分类器处于句子级别，因为它为每个句子计算p（r | x）。奖励是在实例选择器选择的一组新句子上计算的。从本质上讲，上述奖励评估了策略所有行动的整体效用。它监督实例选择器以最大化所选实例的平均可能性，这使得实例选择器的目标函数与关系分类器一致。
在选择过程中，不仅最终行动有助于此奖励，而且所有之前的行动都有。 因此，这种奖励被推迟，并且可以通过强化学习技术很好地处理（Sutton和Barto 1998）。
Optimization优化
对于包B，我们的目标是最大化预期的总奖励。 更正式地说，我们的目标函数被定义为
J(Θ) = VΘ(s0|B) = Es0,a0,s1,a1...,si,ai,si+1[sum(r(si|B)) for i in range(0,|B|+1)]
上式中ai表示第i个Action（πΘ（si，ai）），Si+1表示第i+1个State（P(si+1|si,ai)），过渡式P(si+1|si,ai)等于1，因为状态Si+1完全由状态Si和动作ai确定。VΘ是评价函数，VΘ(s0|B)表示我们从确定状态s0开始依靠策略πΘ（si，ai）遍历B集合后的总期望奖励。
根据策略梯度定理（Sutton等人，1999）和REINFORCE算法（Williams 1992），我们以下列方式计算梯度。对于每个包B，我们根据当前策略顺序地为每个状态采样一个动作。然后我们得到一个采样轨迹{s1，a1，s2，a2，...，s|B|，a|B|，s|B|+1}和相应的最终奖励r（s(|B|+1)|B）。 由于我们只有非零最终奖励，因此从s1到s|B|的所有状态的评价函数都是相同的，即，对于i = 1,2，...，| B |，vi = V（si|B）= r（s(|B|+1)|B）。 我们使用以下渐变更新当前政策：
Θ ← Θ + α(sum(vi∇Θ log πΘ(si,ai)) for i in range(0,|B|))





